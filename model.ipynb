{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 13\n",
    "HIDDEN_UNITS = [128, 64]\n",
    "EPOCHS = 1000\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(npz_path):\n",
    "    data = np.load(npz_path)\n",
    "    X, y = data['x'], data['y']\n",
    "    # Normalize features\n",
    "    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "    if y.ndim == 1:\n",
    "        y = y.reshape(-1, 1)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # He initialization for ReLU\n",
    "        self.weights = np.random.randn(input_size, output_size) * np.sqrt(2 / input_size)\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.z = np.dot(X, self.weights) + self.bias\n",
    "        self.a = relu(self.z)\n",
    "        return self.a\n",
    "    \n",
    "    def backward(self, grad, learning_rate):\n",
    "        delta = grad * relu_derivative(self.z)\n",
    "        dW = np.dot(self.X.T, delta)\n",
    "        db = np.sum(delta, axis=0, keepdims=True)\n",
    "        \n",
    "        # Clip gradients to prevent explosion\n",
    "        dW = np.clip(dW, -1, 1)\n",
    "        db = np.clip(db, -1, 1)\n",
    "        \n",
    "        self.weights -= learning_rate * dW\n",
    "        self.bias -= learning_rate * db\n",
    "        grad_prev = np.dot(delta, self.weights.T)\n",
    "        return grad_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer(Layer):\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.z = np.dot(X, self.weights) + self.bias\n",
    "        return self.z \n",
    "    \n",
    "    def backward(self, grad, learning_rate):\n",
    "        dW = np.dot(self.X.T, grad)\n",
    "        db = np.sum(grad, axis=0, keepdims=True)\n",
    "        \n",
    "        dW = np.clip(dW, -1, 1)\n",
    "        db = np.clip(db, -1, 1)\n",
    "        \n",
    "        self.weights -= learning_rate * dW\n",
    "        self.bias -= learning_rate * db\n",
    "        return np.dot(grad, self.weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size=1):\n",
    "        self.layers = []\n",
    "        sizes = [input_size] + hidden_sizes\n",
    "        for i in range(len(hidden_sizes)):\n",
    "            self.layers.append(Layer(sizes[i], sizes[i+1]))\n",
    "        self.layers.append(OutputLayer(hidden_sizes[-1], output_size))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        a = X\n",
    "        for layer in self.layers:\n",
    "            a = layer.forward(a)\n",
    "        return a\n",
    "    \n",
    "    def backward(self, X, y, learning_rate):\n",
    "        y_pred = self.forward(X)\n",
    "        m = X.shape[0]\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        grad = (y_pred - y) / m\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model( input_size, hidden_units, epochs=1000, lr=0.001):\n",
    "    try:\n",
    "        X_train, y_train = load_data(\"train.npz\")\n",
    "        X_val, y_val = load_data(\"val.npz\")\n",
    "        model = NeuralNetwork(input_size=input_size, hidden_sizes=hidden_units, output_size=1)\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = model.forward(X_train)\n",
    "            loss = np.mean((y_pred - y_train) ** 2)\n",
    "            model.backward(X_train, y_train, lr)\n",
    "            val_pred = model.forward(X_val)\n",
    "            val_loss = np.mean((val_pred - y_val) ** 2)\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch} | Train Loss: {loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    if model is None:\n",
    "        print(\"Model is None. Training failed.\")\n",
    "        return\n",
    "    X_test, y_test = load_data(\"test.npz\")\n",
    "    y_pred = model.forward(X_test)\n",
    "    mae = np.mean(np.abs(y_pred - y_test))\n",
    "    mse = np.mean((y_pred - y_test) ** 2)\n",
    "    print(f\"\\nTest Results:\\nMSE: {mse:.4f}\\nMAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 629.9011 | Val Loss: 626.6075\n",
      "Epoch 100 | Train Loss: 504.2722 | Val Loss: 505.6309\n",
      "Epoch 200 | Train Loss: 386.2849 | Val Loss: 389.7863\n",
      "Epoch 300 | Train Loss: 277.7055 | Val Loss: 283.9707\n",
      "Epoch 400 | Train Loss: 183.5443 | Val Loss: 192.2703\n",
      "Epoch 500 | Train Loss: 113.3449 | Val Loss: 124.4426\n",
      "Epoch 600 | Train Loss: 74.0314 | Val Loss: 86.2232\n",
      "Epoch 700 | Train Loss: 56.9585 | Val Loss: 69.4695\n",
      "Epoch 800 | Train Loss: 47.2022 | Val Loss: 60.0800\n",
      "Epoch 900 | Train Loss: 40.0582 | Val Loss: 53.2258\n",
      "\n",
      "Test Results:\n",
      "MSE: 38.8270\n",
      "MAE: 5.1821\n"
     ]
    }
   ],
   "source": [
    "model = train_model(\n",
    "                       input_size=INPUT_SIZE,\n",
    "                       hidden_units=HIDDEN_UNITS,\n",
    "                       epochs=EPOCHS,\n",
    "                       lr=LEARNING_RATE)\n",
    "    \n",
    "evaluate_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-TF2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
