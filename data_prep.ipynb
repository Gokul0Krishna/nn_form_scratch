{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74c4a31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5cce7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2db6b962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <_PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>, 'test': <_PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>}\n"
     ]
    }
   ],
   "source": [
    "print(mnist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0fa0703",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf=mnist_dataset['train']\n",
    "testdf=mnist_dataset['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1f190a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract one batch (or example) from the dataset\n",
    "for image, label in traindf.take(1):\n",
    "    # Convert TensorFlow tensors to numpy arrays\n",
    "    image_np = image.numpy().squeeze()  # Shape: (28, 28, 1) â†’ (28, 28)\n",
    "    label_np = label.numpy()\n",
    "\n",
    "    # Plot the image\n",
    "    plt.imshow(image_np, cmap='Blues')\n",
    "    plt.title(f\"Label: {label_np}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eb4fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f78cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81fd9f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when creating a layer think about how the array or layer interacts instead of each individual node\n",
    "class Layer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.bias = np.zeros((1, output_dim))#np.zeros creates an array of given length filled with zero\n",
    "    \n",
    "    def forward(self,input):\n",
    "        self.input=input\n",
    "        self.z=(np.dot(input,self.weights)+self.bias)\n",
    "        self.z=relu(self.z)\n",
    "        return self.z\n",
    "    \n",
    "    def backward(self, grad, learning_rate):\n",
    "        delta = grad * relu_derivative(self.z)\n",
    "        dW = np.dot(self.input.T, delta)\n",
    "        db = np.sum(delta, axis=0, keepdims=True)\n",
    "        dW = np.clip(dW, -1, 1)\n",
    "        db = np.clip(db, -1, 1)\n",
    "        self.weights -= learning_rate * dW\n",
    "        self.bias -= learning_rate * db\n",
    "        grad_prev = np.dot(delta, self.weights.T)\n",
    "        return grad_prev\n",
    "    \n",
    "\n",
    "class Outputlayer(Layer):\n",
    "    def forward(self, input):\n",
    "        self.input=input\n",
    "        self.z=(np.dot(input,self.weights)+self.bias)\n",
    "        self.z=softmax(self.z)\n",
    "        return self.z\n",
    "    \n",
    "    def backward(self, grad, learning_rate):\n",
    "        dW = np.dot(self.input.T, grad)\n",
    "        db = np.sum(grad, axis=0, keepdims=True)\n",
    "        dW = np.clip(dW, -1, 1)\n",
    "        db = np.clip(db, -1, 1)\n",
    "        self.weights -= learning_rate * dW\n",
    "        self.bias -= learning_rate * db\n",
    "        return np.dot(grad, self.weights.T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6343ccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size=1):\n",
    "        self.layers = []\n",
    "        sizes = [input_size] + hidden_sizes\n",
    "        for i in range(len(hidden_sizes)):\n",
    "            self.layers.append(Layer(sizes[i], sizes[i+1]))\n",
    "        self.layers.append(Outputlayer(hidden_sizes[-1], output_size))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        a = X\n",
    "        for layer in self.layers:\n",
    "            a = layer.forward(a)\n",
    "        return a\n",
    "    \n",
    "    def backward(self, X, y, learning_rate):\n",
    "        y_pred = self.forward(X)\n",
    "        m = X.shape[0]\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        grad = (y_pred - y) / m\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7543a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model( input_size, hidden_units, epochs=1000, lr=0.001):\n",
    "    X_train, y_train = load_data(\"train.npz\")\n",
    "    X_val, y_val = load_data(\"val.npz\")\n",
    "    model = NeuralNetwork(input_size=input_size, hidden_sizes=hidden_units, output_size=1)\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = model.forward(X_train)\n",
    "        loss = np.mean((y_pred - y_train) ** 2)\n",
    "        model.backward(X_train, y_train, lr)\n",
    "        val_pred = model.forward(X_val)\n",
    "        val_loss = np.mean((val_pred - y_val) ** 2)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch} | Train Loss: {loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9fa1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    X_test, y_test = load_data(\"test.npz\")\n",
    "    y_pred = model.forward(X_test)\n",
    "    loss = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)).mean()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-TF2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
